
@article{davis_university_2011,
	title = {The university of {Florida} sparse matrix collection},
	volume = {38},
	issn = {00983500},
	url = {http://dl.acm.org/citation.cfm?doid=2049662.2049663},
	doi = {10.1145/2049662.2049663},
	language = {en},
	number = {1},
	urldate = {2017-09-29},
	journal = {ACM Transactions on Mathematical Software},
	author = {Davis, Timothy A. and Hu, Yifan},
	month = nov,
	year = {2011},
	pages = {1--25},
	file = {Davis and Hu - 2011 - The university of Florida sparse matrix collection.pdf:/Users/willow/Zotero/storage/WGKGN222/Davis and Hu - 2011 - The university of Florida sparse matrix collection.pdf:application/pdf},
}

@article{higham_accuracy_1993,
	title = {The {Accuracy} of {Floating} {Point} {Summation}},
	volume = {14},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/abs/10.1137/0914050},
	doi = {10.1137/0914050},
	abstract = {The usual recursive summation technique is just one of several ways of computing the sum of n floating point numbers. Five summation methods and their variations are analyzed here. The accuracy of the methods is compared using rounding error analysis and numerical experiments. Four of the methods are shown to be special cases of a general class of methods, and an error analysis is given for this class. No one method is uniformly more accurate than the others, but some guidelines are given on the choice of method in particular cases.},
	number = {4},
	urldate = {2018-06-21},
	journal = {SIAM Journal on Scientific Computing},
	author = {Higham, N.},
	month = jul,
	year = {1993},
	pages = {783--799},
	file = {Higham - 1993 - The Accuracy of Floating Point Summation.pdf:/Users/willow/Zotero/storage/CF29R7WC/Higham - 1993 - The Accuracy of Floating Point Summation.pdf:application/pdf},
}

@article{li_pasta_2019,
	title = {{PASTA}: a parallel sparse tensor algorithm benchmark suite},
	volume = {1},
	issn = {2524-4930},
	shorttitle = {{PASTA}},
	url = {https://doi.org/10.1007/s42514-019-00012-w},
	doi = {10.1007/s42514-019-00012-w},
	abstract = {Tensor methods have gained increasingly attention from various applications, including machine learning, quantum chemistry, healthcare analytics, social network analysis, data mining, and signal processing, to name a few. Sparse tensors and their algorithms become critical to further improve the performance of these methods and enhance the interpretability of their output. This work presents a sparse tensor algorithm benchmark suite (PASTA) for single- and multi-core CPUs. To the best of our knowledge, this is the first benchmark suite for sparse tensor world. PASTA targets on: (1) helping application users to evaluate different computer systems using its representative computational workloads; (2) providing insights to better utilize existed computer architecture and systems and inspiration for the future design. This benchmark suite is publicly released at https://gitlab.com/tensorworld/pasta, under version 0.1.0.},
	language = {en},
	number = {2},
	urldate = {2020-05-12},
	journal = {CCF Transactions on High Performance Computing},
	author = {Li, Jiajia and Ma, Yuchen and Wu, Xiaolong and Li, Ang and Barker, Kevin},
	month = aug,
	year = {2019},
	pages = {111--130},
	file = {Springer Full Text PDF:/Users/willow/Zotero/storage/KFQE2JPW/Li et al. - 2019 - PASTA a parallel sparse tensor algorithm benchmar.pdf:application/pdf},
}

@misc{ieeecomputersociety_sparse_2020,
	title = {A {Sparse} {Tensor} {Benchmark} {Suite} for {CPUs} and {GPUs}},
	url = {https://www.youtube.com/watch?v=sDcgXd8-Rx0},
	urldate = {2022-09-15},
	author = {{ieeeComputerSociety}},
	month = oct,
	year = {2020},
}

@inproceedings{li_sparse_2020,
	title = {A {Sparse} {Tensor} {Benchmark} {Suite} for {CPUs} and {GPUs}},
	doi = {10.1109/IISWC50251.2020.00027},
	abstract = {Tensor computations present significant performance challenges that impact a wide spectrum of applications ranging from machine learning, healthcare analytics, social network analysis, data mining to quantum chemistry and signal processing. Efforts to improve the performance of tensor computations include exploring data layout, execution scheduling, and parallelism in common tensor kernels. This work presents a benchmark suite for arbitrary-order sparse tensor kernels using state-of-the-art tensor formats: coordinate (COO) and hierarchical coordinate (HiCOO) on CPUs and GPUs. It presents a set of reference tensor kernel implementations that are compatible with real-world tensors and power law tensors extended from synthetic graph generation techniques. We also propose Roofline performance models for these kernels to provide insights of computer platforms from sparse tensor view. This benchmark suite along with the synthetic tensor generator is publicly available.},
	booktitle = {2020 {IEEE} {International} {Symposium} on {Workload} {Characterization} ({IISWC})},
	author = {Li, Jiajia and Lakshminarasimhan, Mahesh and Wu, Xiaolong and Li, Ang and Olschanowsky, Catherine and Barker, Kevin},
	month = oct,
	year = {2020},
	keywords = {GPU, Tensors, Sparse matrices, Optimization, Machine learning, Kernel, Benchmark testing, sparse tensors, data analysis, tensor decomposition, benchmarking, Social networking (online)},
	pages = {193--204},
	file = {IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/LFAKBC6M/9251240.html:text/html;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/PCCDP2CM/Li et al. - 2020 - A Sparse Tensor Benchmark Suite for CPUs and GPUs.pdf:application/pdf},
}

@misc{noauthor_willow-ahrenstensordepotjl_nodate,
	title = {willow-ahrens/{TensorDepot}.jl},
	url = {https://github.com/willow-ahrens/TensorDepot.jl},
	urldate = {2022-07-22},
	file = {willow-ahrens/TensorDepot.jl:/Users/willow/Zotero/storage/S97CCVA9/TensorDepot.html:text/html},
}

@misc{noauthor_graph_nodate,
	title = {Graph 500 {\textbar} large-scale benchmarks},
	url = {https://graph500.org/},
	abstract = {Comprehensive benchmarks to address three application kernels: concurrent search, optimization, and edge-oriented.},
	language = {en-US},
	urldate = {2022-07-22},
	file = {Snapshot:/Users/willow/Zotero/storage/5LF45GQD/graph500.org.html:text/html},
}

@techreport{lothian_graph_2013,
	title = {Graph {Generator} {Survey}},
	url = {https://www.osti.gov/biblio/1122669},
	abstract = {The benchmarking effort within the Extreme Scale Systems Center at Oak Ridge National Laboratory seeks to provide High Performance Computing benchmarks and test suites of interest to the DoD sponsor. The work described in this report is a part of the effort focusing on graph generation. A previously developed benchmark, SystemBurn, allowed the emulation of different application behavior profiles within a single framework. To complement this effort, similar capabilities are desired for graph-centric problems. This report examines existing synthetic graph generator implementations in preparation for further study on the properties of their generated synthetic graphs.},
	language = {English},
	number = {ORNL/TM-2013/339},
	urldate = {2022-07-22},
	institution = {Oak Ridge National Lab. (ORNL), Oak Ridge, TN (United States)},
	author = {Lothian, Joshua and Powers, Sarah S. and Sullivan, Blair D. and Baker, Matthew B. and Schrock, Jonathan and Poole, Stephen W.},
	month = oct,
	year = {2013},
	doi = {10.2172/1122669},
	file = {Full Text PDF:/Users/willow/Zotero/storage/G2WKRTAT/Lothian et al. - 2013 - Graph Generator Survey.pdf:application/pdf;Snapshot:/Users/willow/Zotero/storage/A3XTD5HF/1122669.html:text/html},
}

@incollection{chakrabarti_r-mat_2004,
	series = {Proceedings},
	title = {R-{MAT}: {A} {Recursive} {Model} for {Graph} {Mining}},
	isbn = {978-0-89871-568-2},
	shorttitle = {R-{MAT}},
	url = {https://epubs.siam.org/doi/10.1137/1.9781611972740.43},
	abstract = {Graph representation learning (also called graph embeddings) is a popular technique for incorporating network structure into machine learning models. Unsupervised graph embedding methods aim to capture graph structure by learning a low-dimensional vector representation (the embedding) for each node. Despite the widespread use of these embeddings for a variety of downstream transductive machine learning tasks, there is little principled analysis of the effectiveness of this approach for common tasks. In this work, we provide an empirical and theoretical analysis for the performance of a class of embeddings on the common task of pairwise community labeling. This is a binary variant of the classic community detection problem, which seeks to build a classifier to determine whether a pair of vertices participate in a community. In line with our goal of foundational understanding, we focus on a popular class of unsupervised embedding techniques that learn low rank factorizations of a vertex proximity matrix (this class includes methods like GraRep, Deep-Walk, node2vec, NetMF). We perform detailed empirical analysis for community labeling over a variety of real and synthetic graphs with ground truth. In all cases we studied, the models trained from embedding features perform poorly on community labeling. In constrast, a simple logistic model with classic graph structural features handily outperforms the embedding models. For a more principled understanding, we provide a theoretical analysis for the (in)effectiveness of these embeddings in capturing the community structure. We formally prove that popular low-dimensional factorization methods either cannot produce community structure, or can only produce “unstable” communities. These communities are inherently unstable under small perturbations. This theoretical result suggests that even though “good” factorizations exist, they are unlikely to be found by computational methods.},
	urldate = {2022-07-22},
	booktitle = {Proceedings of the 2004 {SIAM} {International} {Conference} on {Data} {Mining} ({SDM})},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Chakrabarti, Deepayan and Zhan, Yiping and Faloutsos, Christos},
	month = apr,
	year = {2004},
	doi = {10.1137/1.9781611972740.43},
	pages = {442--446},
	file = {Full Text:/Users/willow/Zotero/storage/WQB42ZGY/Chakrabarti et al. - 2004 - R-MAT A Recursive Model for Graph Mining.pdf:application/pdf},
}

@inproceedings{leskovec_scalable_2007,
	address = {New York, NY, USA},
	series = {{ICML} '07},
	title = {Scalable modeling of real graphs using {Kronecker} multiplication},
	isbn = {978-1-59593-793-3},
	url = {https://doi.org/10.1145/1273496.1273559},
	doi = {10.1145/1273496.1273559},
	abstract = {Given a large, real graph, how can we generate a synthetic graph that matches its properties, i.e., it has similar degree distribution, similar (small) diameter, similar spectrum, etc? We propose to use "Kronecker graphs", which naturally obey all of the above properties, and we present KronFit, a fast and scalable algorithm for fitting the Kronecker graph generation model to real networks. A naive approach to fitting would take super-exponential time. In contrast, KronFit takes linear time, by exploiting the structure of Kronecker product and by using sampling. Experiments on large real and synthetic graphs show that KronFit indeed mimics very well the patterns found in the target graphs. Once fitted, the model parameters and the resulting synthetic graphs can be used for anonymization, extrapolations, and graph summarization.},
	urldate = {2022-07-22},
	booktitle = {Proceedings of the 24th international conference on {Machine} learning},
	publisher = {Association for Computing Machinery},
	author = {Leskovec, Jure and Faloutsos, Christos},
	month = jun,
	year = {2007},
	pages = {497--504},
	file = {Full Text PDF:/Users/willow/Zotero/storage/2WDUZNCK/Leskovec and Faloutsos - 2007 - Scalable modeling of real graphs using Kronecker m.pdf:application/pdf},
}

@inproceedings{ahrens_autoscheduling_2022,
	address = {New York, NY, USA},
	series = {{PLDI} 2022},
	title = {Autoscheduling for sparse tensor algebra with an asymptotic cost model},
	copyright = {All rights reserved},
	isbn = {978-1-4503-9265-5},
	url = {https://doi.org/10.1145/3519939.3523442},
	doi = {10.1145/3519939.3523442},
	abstract = {While loop reordering and fusion can make big impacts on the constant-factor performance of dense tensor programs, the effects on sparse tensor programs are asymptotic, often leading to orders of magnitude performance differences in practice. Sparse tensors also introduce a choice of compressed storage formats that can have asymptotic effects. Research into sparse tensor compilers has led to simplified languages that express these tradeoffs, but the user is expected to provide a schedule that makes the decisions. This is challenging because schedulers must anticipate the interaction between sparse formats, loop structure, potential sparsity patterns, and the compiler itself. Automating this decision making process stands to finally make sparse tensor compilers accessible to end users. We present, to the best of our knowledge, the first automatic asymptotic scheduler for sparse tensor programs. We provide an approach to abstractly represent the asymptotic cost of schedules and to choose between them. We narrow down the search space to a manageably small Pareto frontier of asymptotically non-dominating kernels. We test our approach by compiling these kernels with the TACO sparse tensor compiler and comparing them with those generated with the default TACO schedules. Our results show that our approach reduces the scheduling space by orders of magnitude and that the generated kernels perform asymptotically better than those generated using the default schedules.},
	urldate = {2022-07-21},
	booktitle = {Proceedings of the 43rd {ACM} {SIGPLAN} {International} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Ahrens, Willow and Kjolstad, Fredrik and Amarasinghe, Saman},
	month = jun,
	year = {2022},
	keywords = {Compilers, Asymptotic Analysis, Automatic Scheduling, Conjunctive Query Containment, Query Optimization, Sparse Tensors},
	pages = {269--285},
	file = {Full Text PDF:/Users/willow/Zotero/storage/V6PAKBZT/Ahrens et al. - 2022 - Autoscheduling for sparse tensor algebra with an a.pdf:application/pdf;Full Text PDF:/Users/willow/Zotero/storage/QDCN3ULM/Ahrens et al. - 2022 - Autoscheduling for sparse tensor algebra with an a.pdf:application/pdf},
}

@article{seshadhri_-depth_2013,
	title = {An in-depth analysis of stochastic {Kronecker} graphs},
	volume = {60},
	issn = {0004-5411},
	url = {https://doi.org/10.1145/2450142.2450149},
	doi = {10.1145/2450142.2450149},
	abstract = {Graph analysis is playing an increasingly important role in science and industry. Due to numerous limitations in sharing real-world graphs, models for generating massive graphs are critical for developing better algorithms. In this article, we analyze the stochastic Kronecker graph model (SKG), which is the foundation of the Graph500 supercomputer benchmark due to its favorable properties and easy parallelization. Our goal is to provide a deeper understanding of the parameters and properties of this model so that its functionality as a benchmark is increased. We develop a rigorous mathematical analysis that shows this model cannot generate a power-law distribution or even a lognormal distribution. However, we formalize an enhanced version of the SKG model that uses random noise for smoothing. We prove both in theory and in practice that this enhancement leads to a lognormal distribution. Additionally, we provide a precise analysis of isolated vertices, showing that the graphs that are produced by SKG might be quite different than intended. For example, between 50\% and 75\% of the vertices in the Graph500 benchmarks will be isolated. Finally, we show that this model tends to produce extremely small core numbers (compared to most social networks and other real graphs) for common parameter choices.},
	number = {2},
	urldate = {2022-07-22},
	journal = {Journal of the ACM},
	author = {Seshadhri, C. and Pinar, Ali and Kolda, Tamara G.},
	month = may,
	year = {2013},
	keywords = {graph models, Graph500, R-MAT, Stochastic Kronecker Graphs (SKG)},
	pages = {13:1--13:32},
	file = {Full Text PDF:/Users/willow/Zotero/storage/ZU5VDJWX/Seshadhri et al. - 2013 - An in-depth analysis of stochastic Kronecker graph.pdf:application/pdf},
}

@article{hubschle-schneider_linear_2020,
	title = {Linear work generation of {R}-{MAT} graphs},
	volume = {8},
	issn = {2050-1242, 2050-1250},
	url = {https://www.cambridge.org/core/journals/network-science/article/linear-work-generation-of-rmat-graphs/68A0DDA58A7B84E9B3ACA2DBB123A16C},
	doi = {10.1017/nws.2020.21},
	abstract = {R-MAT (for Recursive MATrix) is a simple, widely used model for generating graphs with a power law degree distribution, a small diameter, and communitys structure. It is particularly attractive for generating very large graphs because edges can be generated independently by an arbitrary number of processors. However, current R-MAT generators need time logarithmic in the number of nodes for generating an edge— constant time for generating one bit at a time for node IDs of the connected nodes. We achieve constant time per edge by precomputing pieces of node IDs of logarithmic length. Using an alias table data structure, these pieces can then be sampled in constant time. This simple technique leads to practical improvements by an order of magnitude. This further pushes the limits of attainable graph size and makes generation overhead negligible in most situations.},
	language = {en},
	number = {4},
	urldate = {2022-07-22},
	journal = {Network Science},
	author = {Hübschle-Schneider, Lorenz and Sanders, Peter},
	month = dec,
	year = {2020},
	note = {Publisher: Cambridge University Press},
	keywords = {parallel processing, sampling, bit parallelism, graph generator, large graphs},
	pages = {543--550},
	file = {Full Text PDF:/Users/willow/Zotero/storage/FYYDV5DS/Hübschle-Schneider and Sanders - 2020 - Linear work generation of R-MAT graphs.pdf:application/pdf;Snapshot:/Users/willow/Zotero/storage/J44XZ32Y/68A0DDA58A7B84E9B3ACA2DBB123A16C.html:text/html},
}

@article{langr_evaluation_2016,
	title = {Evaluation {Criteria} for {Sparse} {Matrix} {Storage} {Formats}},
	volume = {27},
	issn = {1558-2183},
	doi = {10.1109/TPDS.2015.2401575},
	abstract = {When authors present new storage formats for sparse matrices, they usually focus mainly on a single evaluation criterion, which is the performance of sparse matrix-vector multiplication (SpMV) in FLOPS. Though such an evaluation is essential, it does not allow to directly compare the presented format with its competitors. Moreover, in case that matrices are within an HPC application constructed in different formats, this criterion alone is not sufficient for the key decision whether or not to convert them into the presented format for the SpMV-based application phase. We establish ten evaluation criteria for sparse matrix storage formats, discuss their advantages and disadvantages, and provide general suggestions for format authors/evaluators to make their work more valuable for the HPC community.},
	number = {2},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Langr, Daniel and Tvrdík, Pavel},
	month = feb,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Parallel and Distributed Systems},
	keywords = {sparse matrix-vector multiplication, Sparse matrices, Runtime, parallel processing, Indexes, matrix-vector multiplication, sparse matrix, Matrix converters, mathematics computing, test matrices, Standards, vectors, storage format, matrix algebra, Memory management, evaluation criteria, Evaluation criterion, FLOPS, HPC application, memory footprint, nonzero matrix structure, single evaluation criterion, sparse matrix storage formats, SpMV-based application phase},
	pages = {428--440},
	file = {IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/DRZMJ4TJ/7036061.html:text/html;IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/D75SXF9Z/7036061.html:text/html;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/8IRGEGDG/Langr and Tvrdík - 2016 - Evaluation Criteria for Sparse Matrix Storage Form.pdf:application/pdf;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/I3FNKNCF/Langr and Tvrdík - 2016 - Evaluation Criteria for Sparse Matrix Storage Form.pdf:application/pdf},
}

@misc{smith_frostt_2017,
	title = {{FROSTT}: {The} {Formidable} {Repository} of {Open} {Sparse} {Tensors} and {Tools}},
	url = {http://frostt.io/},
	author = {Smith, Shaden and Choi, Jee W. and Li, Jiajia and Vuduc, Richard and Park, Jongsoo and Liu, Xing and Karypis, George},
	year = {2017},
}

@inproceedings{vuduc_performance_2002,
	title = {Performance {Optimizations} and {Bounds} for {Sparse} {Matrix}-{Vector} {Multiply}},
	doi = {10.1109/SC.2002.10025},
	abstract = {We consider performance tuning, by code and data structure reorganization, of sparse matrix-vector multiply (SpM×V), one of the most important computational kernels in scientific applications. This paper addresses the fundamental questions of what limits exist on such performance tuning, and how closely tuned code approaches these limits. Specifically, we develop upper and lower bounds on the performance (Mflop/s) of SpM×V when tuned using our previously proposed register blocking optimization. These bounds are based on the non-zero pattern in the matrix and the cost of basic memory operations, such as cache hits and misses. We evaluate our tuned implementations with respect to these bounds using hardware counter data on 4 different platforms and on test set of 44 sparse matrices. We find that we can often get within 20\% of the upper bound, particularly on class of matrices from finite element modeling (FEM) problems; on non-FEM matrices, performance improvements of 2× are still possible. Lastly, we present new heuristic that selects optimal or near-optimal register block sizes (the key tuning parameters) more accurately than our previous heuristic. Using the new heuristic, we show improvements in SpM×V performance (Mflop/s) by as much as 2.5× over an untuned implementation. Collectively, our results suggest that future performance improvements, beyond those that we have already demonstrated for SpM×V, will come from two sources: (1) consideration of higher-level matrix structures (e.g. exploiting symmetry, matrix reordering, multiple register block sizes), and (2) optimizing kernels with more opportunity for data reuse (e.g. sparse matrix-multiple vector multiply, multiplication of AT A by a vector).},
	booktitle = {{SC} '02: {Proceedings} of the 2002 {ACM}/{IEEE} {Conference} on {Supercomputing}},
	author = {Vuduc, R. and Demmel, J.W. and Yelick, K.A. and Kamil, S. and Nishtala, R. and Lee, B.},
	month = nov,
	year = {2002},
	note = {ISSN: 1063-9535},
	keywords = {Data structures, Hardware, Sparse matrices, Testing, Optimization, Upper bound, Kernel, Costs, Registers, Counting circuits},
	pages = {26--26},
	file = {IEEE Xplore Abstract Record:/Users/willow/Zotero/storage/NB49QD9N/1592862.html:text/html;IEEE Xplore Full Text PDF:/Users/willow/Zotero/storage/BLUY9Q7P/Vuduc et al. - 2002 - Performance Optimizations and Bounds for Sparse Ma.pdf:application/pdf;Vuduc et al. - 2002 - Performance Optimizations and Bounds for Sparse Ma.pdf:/Users/willow/Zotero/storage/EIBMBAFB/Vuduc et al. - 2002 - Performance Optimizations and Bounds for Sparse Ma.pdf:application/pdf},
}

@misc{beamer_gap_2017,
	title = {The {GAP} {Benchmark} {Suite}},
	url = {http://arxiv.org/abs/1508.03619},
	abstract = {We present a graph processing benchmark suite with the goal of helping to standardize graph processing evaluations. Fewer diﬀerences between graph processing evaluations will make it easier to compare diﬀerent research eﬀorts and quantify improvements. The benchmark not only speciﬁes graph kernels, input graphs, and evaluation methodologies, but it also provides optimized reference implementations. These reference implementations are representative of state-of-the-art performance, and thus new contributions should outperform them to demonstrate an improvement.},
	language = {en},
	urldate = {2023-06-13},
	publisher = {arXiv},
	author = {Beamer, Scott and Asanović, Krste and Patterson, David},
	month = may,
	year = {2017},
	note = {arXiv:1508.03619 [cs]},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Beamer et al. - 2017 - The GAP Benchmark Suite.pdf:/Users/willow/Zotero/storage/YJI9BUWX/Beamer et al. - 2017 - The GAP Benchmark Suite.pdf:application/pdf},
}

@misc{noauthor_sparsebench_nodate,
	title = {{SparseBench} home page},
	url = {https://netlib.org/benchmark/sparsebench/},
	urldate = {2023-06-13},
	file = {SparseBench home page:/Users/willow/Zotero/storage/TYCE2XNQ/sparsebench.html:text/html},
}

@inproceedings{stathis_d-sab_2003,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {D-{SAB}: {A} {Sparse} {Matrix} {Benchmark} {Suite}},
	isbn = {978-3-540-45145-7},
	shorttitle = {D-{SAB}},
	doi = {10.1007/978-3-540-45145-7_52},
	abstract = {In this paper we present the Delft Sparse Architecture Benchmark (D-SAB) Suite for evaluating sparse matrice architectures. The focus is on providing a benchmark suite which is flexible and easy to port on (novel) systems, yet complete enough to expose the main difficulties which are encountered when dealing with sparse matrices. The novelty compared to previous benchmarks is that it is not limited by the need for a compiler. The D-SAB comprises of two parts: (1) the benchmark algorithms and (2) the sparse matrix set. The benchmark algorithms (operations) are categorized in (a) value related operations and (b) position related operations.},
	language = {en},
	booktitle = {Parallel {Computing} {Technologies}},
	publisher = {Springer},
	author = {Stathis, Pyrrhos and Vassiliadis, Stamatis and Cotofana, Sorin},
	editor = {Malyshkin, Victor E.},
	year = {2003},
	keywords = {Sparse Matrix, Sparse Matrice, Benchmark Algorithm, Benchmark Suite, Sparsity Pattern},
	pages = {549--554},
	file = {Full Text PDF:/Users/willow/Zotero/storage/M9XJWEWU/Stathis et al. - 2003 - D-SAB A Sparse Matrix Benchmark Suite.pdf:application/pdf},
}

@misc{noauthor_graphblasbinsparse-specification_nodate,
	title = {{GraphBLAS}/binsparse-specification: {A} cross-platform binary storage format for sparse data, particularly sparse matrices.},
	url = {https://github.com/GraphBLAS/binsparse-specification},
	urldate = {2023-09-12},
	file = {GraphBLAS/binsparse-specification\: A cross-platform binary storage format for sparse data, particularly sparse matrices.:/Users/willow/Zotero/storage/LSHSHLUL/binsparse-specification.html:text/html},
}

@misc{noauthor_numpyload_nodate,
	title = {numpy.load — {NumPy} v1.25 {Manual}},
	url = {https://numpy.org/doc/stable/reference/generated/numpy.load.html},
	urldate = {2023-09-12},
	file = {numpy.load — NumPy v1.25 Manual:/Users/willow/Zotero/storage/6SU3Q2VJ/numpy.load.html:text/html},
}

@misc{noauthor_hdf5_nodate,
	title = {The {HDF5}® {Library} \& {File} {Format}},
	url = {https://www.hdfgroup.org/solutions/hdf5/},
	language = {en-US},
	urldate = {2023-09-12},
	journal = {The HDF Group},
	file = {Snapshot:/Users/willow/Zotero/storage/Z49BNTL8/hdf5.html:text/html},
}

@misc{noauthor_evaluation_nodate,
	title = {Evaluation {Criteria} for {Sparse} {Matrix} {Storage} {Formats}},
	url = {https://ieeexplore.ieee.org/document/7036061},
	abstract = {When authors present new storage formats for sparse matrices, they usually focus mainly on a single evaluation criterion, which is the performance of sparse matrix-vector multiplication (SpMV) in FLOPS. Though such an evaluation is essential, it does not allow to directly compare the presented format with its competitors. Moreover, in case that matrices are within an HPC application constructed in different formats, this criterion alone is not sufficient for the key decision whether or not to convert them into the presented format for the SpMV-based application phase. We establish ten evaluation criteria for sparse matrix storage formats, discuss their advantages and disadvantages, and provide general suggestions for format authors/evaluators to make their work more valuable for the HPC community.},
	language = {en-US},
	urldate = {2023-10-03},
	file = {Snapshot:/Users/willow/Zotero/storage/VC6QEXZY/7036061.html:text/html},
}

@misc{gao_systematic_2024,
	title = {A {Systematic} {Literature} {Survey} of {Sparse} {Matrix}-{Vector} {Multiplication}},
	url = {http://arxiv.org/abs/2404.06047},
	doi = {10.48550/arXiv.2404.06047},
	abstract = {Sparse matrix-vector multiplication (SpMV) is a crucial computing kernel with widespread applications in iterative algorithms. Over the past decades, research on SpMV optimization has made remarkable strides, giving rise to various optimization contributions. However, the comprehensive and systematic literature survey that introduces, analyzes, discusses, and summarizes the advancements of SpMV in recent years is currently lacking. Aiming to fill this gap, this paper compares existing techniques and analyzes their strengths and weaknesses. We begin by highlighting two representative applications of SpMV, then conduct an in-depth overview of the important techniques that optimize SpMV on modern architectures, which we specifically classify as classic, auto-tuning, machine learning, and mixed-precision-based optimization. We also elaborate on the hardware-based architectures, including CPU, GPU, FPGA, processing in Memory, heterogeneous, and distributed platforms. We present a comprehensive experimental evaluation that compares the performance of state-of-the-art SpMV implementations. Based on our findings, we identify several challenges and point out future research directions. This survey is intended to provide researchers with a comprehensive understanding of SpMV optimization on modern architectures and provide guidance for future work.},
	urldate = {2024-04-19},
	publisher = {arXiv},
	author = {Gao, Jianhua and Liu, Bingjie and Ji, Weixing and Huang, Hua},
	month = apr,
	year = {2024},
	note = {arXiv:2404.06047 [cs]},
	keywords = {68-02, 68W10, 65F50, A.1, Computer Science - Distributed, Parallel, and Cluster Computing, D.1.3, G.1.3},
	file = {arXiv Fulltext PDF:/Users/willow/Zotero/storage/5YHN4GG2/Gao et al. - 2024 - A Systematic Literature Survey of Sparse Matrix-Ve.pdf:application/pdf;arXiv.org Snapshot:/Users/willow/Zotero/storage/NHV574M9/2404.html:text/html},
}

@inproceedings{yesil_dense_2022,
	address = {Virtual Event},
	title = {Dense dynamic blocks: optimizing {SpMM} for processors with vector and matrix units using machine learning techniques},
	isbn = {978-1-4503-9281-5},
	shorttitle = {Dense dynamic blocks},
	url = {https://dl.acm.org/doi/10.1145/3524059.3532369},
	doi = {10.1145/3524059.3532369},
	abstract = {Recent processors have been augmented with matrix-multiply units that operate on small matrices, creating a functional unit-rich environment. These units have been successfully employed on dense matrix operations such as those found in the Basic Linear Algebra Subprograms (BLAS). In this work, we exploit these new matrixmultiply facilities to speed up Sparse Matrix Dense Matrix Multiplications (SpMM) for highly sparse matrices.},
	language = {en},
	urldate = {2024-04-19},
	booktitle = {Proceedings of the 36th {ACM} {International} {Conference} on {Supercomputing}},
	publisher = {ACM},
	author = {Yesil, Serif and Moreira, José E. and Torrellas, Josep},
	month = jun,
	year = {2022},
	pages = {1--14},
	file = {Yesil et al. - 2022 - Dense dynamic blocks optimizing SpMM for processo.pdf:/Users/willow/Zotero/storage/CFCCZBRP/Yesil et al. - 2022 - Dense dynamic blocks optimizing SpMM for processo.pdf:application/pdf},
}
