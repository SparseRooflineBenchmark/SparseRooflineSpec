\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{hyperref}

\usepackage[many]{tcolorbox}
\newcommand {\grace}[1]{{\color{violet}\sf{[Grace: #1]}}}
\newcommand {\willow}[1]{{\color{green}\sf{[Willow: #1]}}}
\newcommand {\olivia}[1]{{\color{red}\sf{[Olivia: #1]}}}
\newcommand {\dan}[1]{{\color{orange}\sf{[Dan: #1]}}}
\newcommand {\glb}[1]{{\color{blue}\sf{[Gilbert: #1]}}}
\newcommand {\jim}[1]{{\color{purple}\sf{[Jim: #1]}}}

\title{The Sparse Roofline Benchmark Specification}
\author{Editors: Willow Ahrens, Olivia Hsu, (Please add your name!)}
\date{June 2023}

\begin{document}

\maketitle

\section{Introduction}

We will discuss the introduction at a later meeting after previous comments have been incorporated into a draft.

%Sparse matrix and tensor problems have received increased attention in recent years. We have developed this benchmark suite to encourage meaningful comparison among the varieties of solutions.
%Direct comparisons in sparse computing research are difficult due to the wide variety of sparse kernels, input-dependent performance, differences in intended use cases for each kernel, and a lack of realistic or theoretically interesting datasets.

%Prior benchmark suites have encouraged competition to improve the performance of various subsets or combinations of sparse kernels. SparseBench was proposed by the Netlib to measure numerical improvements to sparse iterative kernels that appear in scientific computing applications. The GAP (Graph Algorithm
%Platform) benchmark suite describes entire graph applications composed of multiple sparse kernels. PASTA (A Parallel Sparse Tensor Algorithm Benchmark Suite) was proposed to measure sparse tensor kernels, but only concerns kernels with a single sparse tensor. 

%Our benchmark suite aims to measure a set of specific sparse kernels that completely covers the sparse performance engineering landscape. This requires several novel features.

%First, we specify datasets or generators for all of the inputs to a given kernel, including situations where multiple inputs are sparse and their patterns interact. As an example, this situation arises in sparse matrix times sparse matrix multiply, where the sparsity of the output is not known at the outset and may belong to different distributions depending on how the inputs correlate. 

%Second, we specify explicit staging of sparse kernels to quantify the tradeoff between preprocessing time and runtime. Many optimizations to SpMV, for example, involve reformatting and reorganizing the matrix before multiplying it, under the assumption it will be used multiple times.

%Third, we provide sparse tensor statistics for each problem (such as the sparsity of the output or the number of required operations), and generators for pathological cases. This helps quantify when implementations are reaching their theoretical limit, the roofline.

%Our benchmark suite aims to provide researchers with the software and specifications to easily benchmark sparse problems on realistic datasets across multiple languages and architectures. We also provide an easily accessible database to tabulate the results: the best known approaches to sparse problems.

%Note: There is more potential text for the introduction at the end of the document

\section{Reference Implementation}

While the precise mathematical specification of the computation is specified in this document, we have also provided a reference implementation for each problem.
%We will produce a framework for running our benchmarks on various target hardwares, providing data for various backends. The framework will have places to plug in a candidate implementation, and the script will output the runtimes for appropriate systems.

\section{Tensor Formats}

To enable cross-language compatibility, all tensors generators or dataset downloads will produce tensor files in Binsparse tensor formats. The precise filenames and paths will be specified along with the benchmark kernel that uses them. For example, one might use the RMAT matrix generator included with the suite to generate inputs, stored in \texttt{spmv/RMAT/A.ttx} and \texttt{spmv/RMAT/x.ttx}

\glb{make sure to precisely specify what "dense" format means here.}

\section{Timing}

The time to transfer input/output data to and from disk should not be measured.  Timing should begin with inputs resident in memory with the specified binary format.  (see Format Section)  The timing should end after the output is resident in memory with the specified binary format.

How will we account for memory movement (e.g. if I have a task to be dispatched from CPU to GPU, do we include the data movement cost?)

\subsection{Staging}

Each benchmark is separated into stages based on which inputs the application is allowed to read at each stage. For example, to enable a clear distinction between matrix preprocessing and runtime in SpMV kernels, the first stage allows access to just the matrix. Any vector-agnostic preprocessing can be timed within the first stage. The second stage allows access to the vector, and thus computation of the output. We recognize that many divisions between stages are possible; we have selected separations into stages that we believe are meaningful for applications.

\glb{Need to put some more thought into how results should be aggregated/specified as well as what kind of benchmarking practices we either require or trust users to use.}
Applications may choose to report either the minimum or the geometric mean of a predefined number of exections, and should also report whether the cache was cleared at the start of each execution.

\section{Correctness and Validation}

\subsection{Exception Handling}
\begin{itemize}
\item NaN * Explicit zero = NaN

\item NaN * implicit zero = implicit zero
\end{itemize}

\subsection{Comparison to Reference Implementation}
\begin{enumerate}
    \item Allow reorganization of algorithm up to equivalence under arbitrary semirings (treat unsupported functions as uninterpreted) (e.g. thou shalt not do Strassen)
    \item Allow reorganization of algorithm up to equivalence under infinite precision
    \item Some combination of the above
    \item Anything you want up to declared empirical accuracy
\end{enumerate}

If algorithm is iterative, run steps until tolerance
If algorithm is non-iterative, report achieved empirical accuracy compared to double.

\section{In Memory Formats}


\section{Datasets}

What makes a choice of dataset a good choice?
\begin{itemize}
    \item There is a citation we can make
    \item There is a clear story/good argument connecting this data and its use (on a given kernel) to an application that people care about.
    \item The dataset is taken from another already existing benchmark
    \item The dataset exercises a particular extremal complexity case (e.g. expander graphs, or things coming from the lower bounds group)
    \item That the dataset should be representative of a real application of a  kernel (i.e. make sense together)
\end{itemize}

\subsection{Tensor Datasets}

	Tensors may be generated by a generator, or downloaded from a dataset
	online. The tensor datasets are listed here, together with their keys. For
	example, the key \texttt{SuiteSparse/Boeing/ct20stif.bs} refers to the
	\texttt{Boeing/ct20stif} matrix in the \texttt{SuiteSparse} collection. The
	\texttt{download.jl} script included with the suite will download and reformat
	datasets to binsparse or TTX format.

	\begin{itemize}
		\item[\texttt{SuiteSparse}] The University of Florida SuiteSparse Matrix Collection \cite{davis_university_2011}
		\item[\texttt{FROSTT}] The Formiddable Repository of Open Sparse Tensors and Tools \cite{smith_frostt_2017}
		\item[\texttt{ImageNet}] \url{https://www.kaggle.com/competitions/imagenet-object-localization-challenge/}
	\end{itemize}

\subsection{Tensor Generators}

    Tensor generators are listed here, together with a description of their
    parameters. All tensor generators that use random numbers additionally take a
	seed argument $s$. 

\subsubsection{RMAT}
    The RMAT matrix generator \cite{chakrabarti_r-mat_2004} creates graphs with
    power-law degree distributions, which presents an interesting challenge to
    load balancing for parallelization. The generator works by taking the
    Kronecker product of a seed matrix with itself several times to produce a
    matrix of probabilities of including nonzeros. An artifact of RMAT matrix
    generation is that heavy rows are often grouped together (a "staircase"
    pattern), so we choose to apply a random row/column permutation to the rows
    and columns. RMAT is used in the Graph500 and GAP benchmarks
    \cite{noauthor_graph_nodate, beamer_gap_2017}. Additionally, the tensor
    generalization (which takes Kronecker products of a tensor seed with itself)
    is used in the PASTA Benchmark Suite \cite{li_pasta_2019}.

    The RMAT matrix generator classically uses a generating matrix

	\[
		G = \left[\begin{array}{cc}
			A & B \\
			C & D \\
		\end{array}\right],
	\]
	though we will specify the entire tensor $G$, rather than the parameters
	$A$, $B$, $C$, $D$. Additionally, we will specify $r$, the number of seed
	matrices which should be joined with a Kronecker product.

\section{Kernels}

\subsection{SpMV}

\subsubsection{Input Arguments}

\begin{itemize}
	\item[$m$:] a positive integer.
	\item[$n$:] a positive integer.
	\item[$A$:] an $m \times n$ sparse matrix.
	\item[$x$:] an $n$-length dense vector.
\end{itemize}

\subsubsection{Output Arguments}

\begin{itemize}
	\item[$y$:] an $m$-length dense vector.
\end{itemize}

\subsubsection{Computation}

$$y_i = \sum_{j}A_{ij}x_j$$

\subsubsection{Sparse Format Variants}

\glb{This text should be reduced.  It's just here to suggest}
There should be a list here of options that have already been defined in the format section for $A$ and $x$ (and any other inputs).  In the case of SpMV, this just means specifying format for $A$, because $x$ and $y$ are dense.

https://api.csswg.org/bikeshed/?url=https://raw.githubusercontent.com/GraphBLAS/binsparse-specification/main/spec/latest/index.bs

https://github.com/GraphBLAS/binsparse-specification

\subsubsection{Benchmark Stages (Provide timings for each stage)}

SpMV is frequently used repeatedly on the same matrix. Therefore, it is the
subject of many optimizations which are independent of the vector $x$. Thus, 
we separate the benchmark into two stages:

\begin{itemize}
\item \textbf{(Stage 1)}
	The kernel may read $m$, $n$, and $A$.
\item \textbf{(Stage 2)}:
	The kernel may read $x$.
\end{itemize}

\subsubsection{Datasets}

\begin{itemize}
	\item[\texttt{spmv/kronecker10}:] 100 random kronecker networks at 10 percent sparsity, using the
	same parameters as GAP and Graph500 \cite{noauthor_graph_nodate}.
	\begin{itemize}
		\item[\texttt{spmv/kronecker10/A}:] $\texttt{RMAT}\left(r=27, G = \left[\begin{array}{cc}
			0.57 & 0.19 \\
			0.19 & 0.05 \\
		\end{array}\right]\right)$
		\item[\texttt{spmv/kronecker10/x}:] $\texttt{rand}\left(m = 2^{27}\right)$
	\end{itemize}
	\item[\texttt{spmv/OSKI}:] the sparse matrices used in evaluation of
	``Performance Optimizations and Bounds for Sparse Matrix-Vector Multiply'' \cite{vuduc_performance_2002}.
\end{itemize}

\subsubsection{Variants to Measure}

Here are the combinations of settings that should actually be measured.


Everything after this line is a draft:

\noindent\rule{\textwidth}{1pt}

\subsubsection{Variations}
    Also include SpMV transpose.

    Mention $A=A^T$, possible additional optimizations,
    including``Fast Bilinear Algorithms for Symmetric Tensor 
    Contractions'', E. Solomonik, J. Demmel, Computational Methods
    in Applied Mathematics, Feb 2020.
    \newline
    https://doi.org/10.1515/cmam-2019-0075



\subsection{Iterative (Jacobi? PCG?) Solve}
	Input (Stage 1):
		Matrix given (e.g. “Boeing/ct20stif” or power-law graph laplacian)
	Input (Stage 2):
		Vector (e.g. uniform random dense vector)
	Compute:
		$y_j = A_ij * x_j$
	Timings:
Report time start stage 1 to start stage 2
Report time start stage 2 to end
Report aggregate time
Some combination of:
1. Allow reorganization of algorithm up to equivalence under arbitrary semirings (treat unsupported functions as uninterpreted) (e.g. thou shalt not do Strassen)
2. Allow reorganization of algorithm up to equivalence under infinite precision
2.5 (2 and 3)
3. Anything you want up to declared empirical accuracy
If algorithm is iterative, run steps until tolerance
If algorithm is non-iterative, report achieved empirical accuracy compared to double.

\subsection{SpMSpV}
``Sparse-Sparse Matrix-Vector Multiply'' i.e. the matrices and vectors are all sparse.
$$y_i += A_{i,j} * x_j$$

\subsection{SpMM (SpGEMM?)}
	Input (Stage 1):
		Matrix
	Input (Stage 2):
		Vector
	Compute:
		$y_ij = A_ik * x_kj$
	Timings:
Report time start stage 1 to start stage 2
Report time start stage 2 to end
Report aggregate time

SpGEMM from multi-source BFS:
	Computation: Compute BFS parents starting from a particular set of vertices.
	Distribution: If the graph is fixed, you can precompute the BFS parents (not interesting). Thus, this problem is only well-posed on graph generators. I’m not sure if the set of vertices matters, could pick randomly.

SpGEMM from setup phase of algebraic multigrid:
	Computation: Compute $R_lA_lP_L$ as described in \url{https://doi.org/10.1145/3571157}.
	Distribution: matrices described in \url{https://doi.org/10.1145/3571157}
SpGEMM with various levels of size prediction difficulty and output reuse:
	Computation: Compute C = A*B
	Distribution: Generate A and B (using RMAT, perhaps?) so that expected number of nonzeros in A and B remain fixed but expected number of nonzeros in C can be very high, very low, or highly variable

\subsection{ATA}

\subsection{MTTKRP}
	Computation: A single MTTKRP 
	Distribution: The dense matrices are random, but the tensor can be fixed or random.


\subsection{Candecomp}
	Computation: Some fixed number of MTTKRPs of the same tensor in alternating transposition orders on the same factor matrices
	Distribution: The tensor can be fixed or random here as well.
Stencil (Same as SpMV, but matrix is a stencil)
	Computation: SpMV or Congugate gradient
	Distribution: Matrix is a toeplitz matrix for a particular stencil, vector is random

\subsection{FFT}
	Computation: SpMV
	Distribution: Matrix is an FFT matrix, vector is random

\subsection{Dynamic PageRank}
	Computation: Compute Pagerank of a matrix
	Stage 1.
		The first (n - d) rows and columns of a matrix are given
	Stage 2.
		The final d rows and columns are given
	Computation:
		Do 20 iterations of pagerank
	Notes: This incentivizes the implementation to use the first portion of the matrix to accelerate the computation in stage 2.
(Gilbert: This is just a power iteration? Willow: It is. I was trying to emphasize the dynamic graph kernel problem, where updates to a graph are received in real time and we are trying to maintain some statistics on the graph as the updates come in. Gilbert: I'm not sure page-rank or similar are incrementalized in practice?  Maybe.  Maybe worth considering social network clustering algorithms here instead?)

\subsection{Quantum Circuits}
	Computation: Compute the distribution of the quantum circuits outputs
	Distribution: 1 Tensor for each circuit element (10s-100s total)
Transformer Model:
	Computation: Compute the output of a single encoder block in a transformer model with k attention heads
	Distribution: Randomly initialized weights with some sparsification algorithm applied
 
 \begin{align*}
Q[S, E, K] &= \sum_C(I[S, C] * WQ[C, E, K])\\
K[S, E, K] &= \sum_C(I[S, C] * WK[C, E, K])\\
V[S, E, K] &= \sum_C(I[S, C] * WV[C, E, K])\\
AP[S, S’, K] &= \sum_E(Q[S, E, K] * K[S’, E, K]) / 8\\
Z[S, E, K] &= \sum_S’(Sigmoid(AP[S,S’,K]) * V[S’, E, K])\\
CZ[S, E] &= \sum_k(Z[S, E, K] * WZ[K,E])\\
FF[S,FF] &= Relu(CZ[S,E] * WFF1[E,FF])\\
O[S,E] &= Relu(FF[S,FF] * WFF2[FF,E])\\
\end{align*}

\subsection{CountSat}
	Computation: Compute the number of satisfying solutions for a sat problem. This is a tensor contraction along each variable of the product of all clause tensors according to matching variables.
	Distribution: Randomly generated sat clauses with variables of length 3-10 (Existing Sat datasets? Are random sat problems unsatisfiable w.h.p.?)

\subsection{Graph Neural Networks}
	Computation: Compute one iteration of message passing
	Distribution: Random graph models, random sparsified weights

% WX, BX is a simple affine transform, but it could be NN layers instead
$NeighborMessages[u, v, k’] = A[u, v]*(Sum_k(X[v, k]*WX[k, k’]) +BX[k’])  $

$AggNeighborMessages[u, k’] = Sum_v(NeighborMessage[u, v, k’])$

% This could also be a neural network
$NewX[u, k] = softmax_k(WX’[u,k](X[u, k] +AggNeighborMessages[u,k]) + BX’[k]) $

\subsection{SubGraph Counting}
	Computation: Compute the number of subgraphs of a particular kind (triangle, flower, petal, etc.)
	Distribution: Random graph models, directed vs undirected subvariants

\section{Future Benchmark Verticals}

\subsection{Quantum Chemistry Stuff}
Oh cool, quantum chemistry stuff!

\subsection{Morgue from Earlier Notes}

Produce a List of all Computation Kernels (Acronyms & Definitions)
SpMV? SpMM? SpDM3? SpMSpV? SPGEMM? SpLU? SpTRSV/M?

SpMV:
	“Sparse Matrix-Vector Multiply”
Dense x,y, Sparse A
y[i] += A[i, j] * x[j]
SpMSpV:
	“Sparse-Sparse Matrix-Vector Multiply”
Sparse A, x, y
y[i] += A[i, j] * x[j]
SpGEMM:
	“Sparse-Sparse Matrix Multiply”
Sparse C, A, B, often all matrices are square
C[i, j] += A[i, k] * B[k, j]
Masked SpGEMM:
	Sparse (sometimes Boolean) S, C, Sparse A, B
C[i, j] += S[i, j] * A[i, k] * B[k, j]
SDDMM:
	“Sampled Dense Dense Matrix Multiply”
	Sparse (sometimes Boolean) S, C, Dense A, B
C[i, j] += S[i, j] * A[i, k] * B[k, j]

SpMM:
	“Sparse Matrix Multiply”
Dense C, B, Sparse A, often J small constant
C[i, j] += A[i, k] * B[k, j]

MTTKRP:
	“Matricized Tensor Times Khatri-Rao Product”
Sparse B, Dense A, C, D
A[i, j] += B[i, k, l] * C[k, j] * D[l, j]
SpTTM:
	“Sparse Tensor Times Matrix”
Sparse A, B, Dense C, usually K >> L or L >> K
A[i, j, k] += B[i, j, l] * C[k, l]
SpMTTKRP:
Sparse A, B, C, D
A[i, j] += B[i, k, l] * C[k, j] * D[l, j]
SpLU:
	“Sparse LU Factorization”
Sparse A, produce permutations P, Q, sparse lower triangular L, and sparse upper triangular U
such that PAQ=LU
SpTRSV:
	“Sparse Triangular Solve”
	Sparse triangular matrix A, dense vector y, produce dense vector x such that Ax = y
SpTRSM:
	Sparse triangular matrix A, dense matrix Y, produce dense matrix X such that AX = Y

Willow: I’ve taken a stab at listing some kernel definitions. One might group them broadly by:
	1 argument sparse: SpMV, SpMM, SDDMM,MTTKRP, SpTTM
	2+ argument sparse: SpGEMM, SpMSpV, SpMTTKRP
	Loop carry dependency: SpLU, SpTRSV, SpTRSM

%comment: Here is a copy of the call for comments on this benchmark suite, perhaps we can work it into a motivation section above
%Goals the the benchmark suite:
%To discover the best possible approach
%A well-specified benchmark suite that specifies all relevant aspects of problem setup and measurement to facilitate meaningful comparison
%To provide a complete set of problems and data that exercise all important phenomena of sparse kernels and all realistic applications of sparse kernels.
%Ease of use and encouraging participation across a wide variety of participants, from tensor compiler writers targeting many kernels to hardware developers of a single kernel
	
%Why bother doing this?
%We are providing specifications for repeatable benchmarking, accuracy under reduced precision or semirings, and accounting for preprocessing time. 

%Some other people have created tensor benchmarks: PASTA, GAP, Sparsebench
%Why aren’t these sufficient?
%Well, the people on this project haven’t found them sufficient for the papers they published recently (or did they not even try?)
%PASTA doesn’t have a big enough variety of tensors
%All existing benchmarks only provide a single (sparse) matrix or tensor, but most real problems require more than one input.
%Most sparse benchmarks are focused on providing example data to the detriment of specifying computations and measurements
%Sparse problems are highly dependent on format/data-structure choices and/or preprocessing.  (maybe talk about measures instead?)
%Existing benchmarks 
%Current benchmarks do not collect and archive community progress on key problems.
%Are the problems in current benchmarks bullshit problems that no-one really cares about?  (Are ours?)
%The existing datasets are very good for certain sparse applications, but the variety of applications and approaches today are not well represented.
%List of specific, surprising, deficiencies
%There is not sufficient focus on exploring theoretical limits???
%We have an intellectual interest in the best real-world solutions with architectures that exist today.
%The dense world has a roofline model, it implies a certain workflow for crafting dense kernels and focuses development. The sparse world has no model, we don’t know when to stop optimizing. 
%What are the key problems that distill the essence of what “sparse linear/tensor algebra” is about?

%This will benefit the community because it will make it easier to measure progress by having common benchmarks for projects and papers.

%Our benchmark will include a few key features to support our goals: 
%We’re interested in a broad set of benchmarks; our initial kernels include SpMV, SpMM, SpGEMM, SpMTTKRP, Iterative Solvers, Subgraph Counting, and more!
%We will distribute simple, scalable reference implementations for our broad catalog of kernels. We will also include convenient code to generate inputs and compare against the reference.
%We will curate the results online and provide performance metrics that can be used to assess fractions of peak, such as best-known modeled data movement, matrix partitions, and more!

%Here is the bibliography:
\bibliographystyle{plain}
\bibliography{main}

\end{document}
