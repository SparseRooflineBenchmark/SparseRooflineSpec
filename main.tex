\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{hyperref}

\usepackage[many]{tcolorbox}
\newcommand {\grace}[1]{{\color{violet}\sf{[Grace: #1]}}}
\newcommand {\willow}[1]{{\color{green}\sf{[Willow: #1]}}}
\newcommand {\olivia}[1]{{\color{red}\sf{[Olivia: #1]}}}
\newcommand {\dan}[1]{{\color{orange}\sf{[Dan: #1]}}}
\newcommand {\glb}[1]{{\color{blue}\sf{[Gilbert: #1]}}}
\newcommand {\jim}[1]{{\color{purple}\sf{[Jim: #1]}}}
\newcommand {\hrl}[1]{{\color{cyan}\sf{[Hengrui: #1]}}}
\title{The Sparse Roofline Benchmark Specification}
\author{Willow Ahrens \and
	Olivia Hsu \and
	Kyle Deeds \and
	Grace Dinh \and
	Gilbert Bernstein \and
	(Please add your name!)}
\date{June 2023}

\begin{document}

\maketitle

\section{Introduction}

Sparse matrix and tensor problems are at the heart of many applications, from graph algorithms to scientific computing to machine learning.
%
This has led to intense research interest, spanning several decades and countless approaches \cite{gao_systematic_2024}.
%
Much of the variation in approach is due to the variation in application characteristics.
%
The performance of sparse code depends on nonzero patterns in the data, how the patterns interact, and the implementation choices made by the framework.
%
This leads to a situation where different algorithms are better suited to different appliations, with vastly different properties.
%
The situation is complicated by the recent development of large sparse frameworks and compilers, spanning a multitude of different applications.
%
Unlike in dense computing, where it is relatively simple to bound the peak throughput of an application, there is no universally accepted ``Speed of Light'' for sparse operations.

In situations like these, benchmark suites can clarify the performance landscape by providing a representative and exhaustive metric of comparison.
%
Several benchmark suites have been proposed, but none of them sufficiently characterize an exhaustive, specific study of sparse linear algebra.
%
Direct comparisons in sparse computing research are difficult due to the wide variety of sparse kernels, input-dependent performance, differences in intended use cases for each kernel, and a lack of realistic or theoretically interesting datasets.
%
SparseBench (a Netlib project) focuses on numerical improvements to iterative solvers\cite{noauthor_sparsebench_nodate}.
%
The GAP (Graph Algorithm Platform) benchmark focuses on entire graph applications, composed of multiple sparse kernels\cite{beamer_gap_2017}.
%
PASTA (A Parallel Sparse Tensor Algorithm Benchmark Suite) was proposed to measure sparse tensor kernels, but only concerns kernels with a single sparse tensor\cite{li_pasta_2019}. 
%
In the absense of more focused benchmarks, researchers have also turned to database collections of sparse data, such as the University of Florida Sparse Matrix Collection \cite{davis_university_2011} or the FROSTT sparse tensor collection \cite{smith_frostt_2017}.
%
Unfortunately, these collections only collect single sparse matrices without context, so they are not representative inputs to particular sparse problems, especially problems which concern more than one sparse input.

Despite the staggering magnitude of interest, these exisiting benchmark suites are insufficient to clarify the landscape.
%
They lack sparse-sparse interactions and donâ€™t account for sparsity pattern prepreprocessing time.
%
There is also no benchmark suite that spans a wide enough space of sparse kernels to enable meaningful comparisons between tensor compilers.
%
There is also no collection of best-known results for specific problems.
%
We need a new benchmark suite that exhibits relevant properties and agreed upon measures of comparison.

We propose the Sparse Roofline Benchmark suite to accurately characterize the speed of light for sparse computing.
%
This requires several novel features:
\begin{itemize}
	\item We specify datasets for \textbf{problems}, rather than individual matrices.
	%
	This is necessary because the interaction between the nonzero patterns of sparse inputs has a performance effect.
	%
	For example, in sparse matrix times sparse matrix multiply, the sparsity of the output is not known at the outset and may belong to different distributions depending on how the inputs correlate. 
	\item We separate algorithms and measurement into \textbf{stages} to quantify the tradeoff between preprocessing time and runtime.
	%
	Many optimizations to sparse code, such as autotuning or inspector/executor approaches, make optimization decisions based on the concrete problem instances at runtime.
	%
	For example, several approaches to SpMV involve reformatting and reorganizing the matrix before multiplying it, under the assumption it will be used multiple times.
	\item For each problem, we also provide \textbf{random problem generators}.
	%
	If we only specified concrete problems, rather than random ones, it might be easy to tune hyperparameters or even specific instance-specific optimizations ahead of time, resulting
	in an inaccurate characterization of preprocessing time.
	%
	Random instances provide an opportunity to better test inspector/executor strategies and make distributional observations.
	%
	We also provide generators for pathological cases, to help quantify when implementations are reaching their theoretical limit, the roofline.
	\item
	%
	We also provide sparse tensor statistics for each problem (such as the sparsity of the output or the number of required operations) 
\end{itemize}

Our benchmark suite aims to spans the sparse performance engineering landscape.
%
In particular, our goal is to fully characterize the unique properties of all kernels which can be expressed with an operator-generalized einstein summation, or a mapreduce statement, inspired by array programming frameworks such as TACO or NumPy.

Our benchmark suite aims to provide researchers with the software and specifications to easily benchmark sparse problems on realistic datasets across multiple languages and architectures. We also provide an easily accessible database to tabulate the results: the best known approaches to sparse problems.

%Note: There is more potential text for the introduction at the end of the document

\section{Reference Implementation}

While the precise mathematical specification of the computation is specified in this document, we have also provided a reference implementation for each problem.
%We will produce a framework for running our benchmarks on various target hardwares, providing data for various backends. The framework will have places to plug in a candidate implementation, and the script will output the runtimes for appropriate systems.

\section{Tensor Formats} \label{sec:formats}

To enable cross-language compatibility, all tensors generators or dataset downloads will produce tensor files in Binsparse tensor formats \cite{noauthor_graphblasbinsparse-specification_nodate}. Possible tensor format names such as CSR or COO will refer to Binsparse format codes. Binsparse tensors are composed of several data vectors. On disk, these vectors may be stored in any array data file format such as an HDF5 container or an \texttt{.npy} file. During benchmarking, inputs and output data are expected to correspond to in-memory versions of these same arrays.

For example, the CSR structure involves three vectors, \texttt{pointers\_to\_1}, \texttt{indices\_1}, and \texttt{values}. The column indices of the stored values located in row i are located in the range [\texttt{pointers\_to\_1[i], pointers\_to\_1[i+1]}) in the \texttt{indices\_1} array. The scalar values for each of those stored values is stored in the corresponding index in the values array. On disk, these arrays might be serialized to a text representation, but in memory in C, the data would be referenced by two pointers to integers and one pointer to floating-point numbers.

The precise filenames and paths will be specified along with the benchmark kernel that uses them. For example, one might use the RMAT matrix generator included with the suite to generate inputs, stored in \texttt{spmv/RMAT/A.bsp.h5} and \texttt{spmv/RMAT/x.bsp.h5}
%\hrl{In h5 format, especially in those sparse data filled with zeros, practitioner will use a low-rank approximation (e.g., 'On the Compression of Low Rank Matrices' by Cheng et.al) to store it at the cost of minimal loss of accuracy.}
Subsequent sections of this document may specify that inputs live in ``dense'' formats or ``sparse'' formats. We say that the dense binsparse formats are those that store all possible entries, and sparse formats are those that allow one to store only a subset of the entries.

For extended compatibility, implementations may choose to read the matrix from disk in another format, such as FROSTT, MatrixMarket, or TensorMarket \cite{}, but must of course still convert the data to the correct in-memory binsparse format before benchmarking.

We recognize that there are many possible formats to use for benchmarking, but believe that the 

\section{Timing}

The time to transfer input/output data to and from the input file should not be measured.  Timing should begin with inputs resident in memory with the specified binary format (see Section~\ref{sec:formats}). The timing should end after the output is resident in memory with the specified binary format. This benchmark may apply to a diversity of architectures with different memory hierarchies, from distributed memories to special purpose hardware accelerators. The only requirement is that the outputs are resident in the same memory as the inputs, and that only one copy of the input is used. \grace{Some clarification about how to handle this in the case of accelerators with distinct scratchpads and accumulators should be done. Do scratchpads and accumulators count as 'the same' memory?}

Some accelerators have high bandwidth costs to and from the chip (making timings for problems resident in accelerator memory less realistic for those who wish to use the data off-chip). However, this may be accounted for naturally as the problem size grows to exceed the on-chip memory.

\subsection{Staging}

Each benchmark is separated into stages based on which inputs the application is allowed to read at each stage. For example, to enable a clear distinction between matrix preprocessing and runtime in SpMV kernels, the first stage allows access to just the matrix. Any vector-agnostic preprocessing can be timed within the first stage. The second stage allows access to the vector, and thus computation of the output. We recognize that many divisions between stages are possible; we have selected separations into stages that we believe are meaningful for applications.

\subsection{Results Aggregation}

\subsubsection{Deterministic data}

    In situations where the input sparsity patterns are fixed, applications may run as many executions as they wish, reporting the minimum time. Applications should report whether the cache was cleared at the start of each execution.

\subsubsection{Randomized data}

    In situations where the input sparsity patterns are drawn from a distribution, applications must choose a random seed for the generator, and report the mean and variance of the execution times, as well as confidence intervals for each statistic. Implementations are free to choose how many executions are used, but a confidence interval of at least 95\% is required for the mean time.

    The software implementation provides a convenient command-line utility to compute these statistics, as well as an in-software utility, so implementers need only record the runtime on each random input.

    For each random trial input, implementers should follow the guidelines for deterministic data, taking the minimum of as many execution times as they wish and reporting whether the cache was cleared before each.

\section{Correctness and Validation}

    All kernels in the benchmark suite are composed of one or more statements of the form
    \begin{equation}\label{eq:einsum}
        A_{i...} = \sum B_{j...} \cdot C_{k...} \cdot ... ,
    \end{equation}
    where $i, j, k$ may contain overlapping indicies and the operators $+$ or $\cdot$ may be substituted with other operators.

    \subsection{Allowed Optimizations}
        Reference implementations are provided for all kernels in the benchmark suite. Historically, we recongize that many optimizations have been proposed which are typically excluded from benchmarks, such as Strassen's algorithm. However, a goal of this benchmark is to encourage exploration of the unique properties of sparse kernels that arise from the constraint that one must do "all the same operations" as the reference implementation. We therefore add a category for limited optimizations, and considerations for those who wish to perform further algorithmic optimizations.

    \subsubsection{The Semiring Category}
        Both theoretical lower bounds and popular implementations of sparse kernels, such as GraphBLAS, have noticed that a wide variety of sparse optimizations can be described by the mathematical construct of semirings. Thus our "limited" optimizations may utilize any property derived from operators forming semirings. This is the default category we expect most optimizations to fall under, and includes the associative and commutative property. This does not include the distributive property.  In this categorization, we treat floating point numbers as real numbers for the purpose of applying mathematical properties.
    \subsubsection{Beyond Semirings}
        For implementations that wish to break the above rule, any mathematically equivalent optimization is allowed. This category allows optimizations such as Strassen's algorithm. 

    \subsection{Floating-Point Accuracy}
        Although the previous section treats floating point numbers as real numbers, in practice they behave differently. We do not require any particular exception handling behavior.
        
         \subsubsection{Error Bounds}
            Implementations involving floating point numbers must satisfy a relative error bound on \eqref{eq:einsum} of the following form, where $\epsilon$ is a declared parameter and $n$ is the number of elements in the sum:

            \[
                \forall_{i...} \left|A\_ref_{i...} - A\_res_{i...}\right| \leq n \epsilon \sum |B_{j...} \cdot C_{k...} \cdot ...|
            \]

            If only \texttt{Semigroup} level optimizations are used, then this equation will hold for the default value of machine $\epsilon$.

            This equation is inspired by the standard error bound for recursive summation, taken from \cite{higham_accuracy_1993}, and our generalized form can be adapted to several strategies for summation.

            \subsection{Floating Point Accuracy V2 \jim{This draft is based on the current draft of the Sparse BLAS Standard}}

            The emergence of various arithmetic formats, including mixed precision,
is making it increasingly important for numerical software to carefully
document what accuracy and related properties users can expect,
and for standards committees to evaluate these
claims, through extensive test code. Based on existing formats and
algorithms, we propose 2 error bounds, each depending on 2 parameters.
The two 2 error bounds correspond to using either conventional floating point,
or some variant of block floating point, to accumulate the (sparse) dot products
underlying those SparseBLAS operations that are expressible using 
dot products (we assume no Strassen-like algorithms are used, since these
make the intermediate results much less sparse). The two parameters 
correspond to the values of machine epsilon used to accumulate the dot product, 
and to store the final result, which may be lower in the case of mixed precision. 
The error bounds can of course depend both on the operation, and the input and 
output data types.

We express the overall error bound as follows. $E$ represents the exact result
(of a dot product), $A_1$ the first approximation, and $A_2$ the final approximation,
which may equal $A_1$, or require rounding $A_1$ to lower precision.
Suppose we can bound $|E - A_1| \leq B_1$ (an absolute error bound),
and $|A_2 - A_1| \leq B_2 \dot |A_1|$ (a relative error bound, so $B_2 \ll 1$). 
Then the overall bound is
\begin{eqnarray*}
|E - A_2| & \leq & |E - A_1| + |A_1 - A_2| 
            \leq B_1 + B_2 \cdot |A_1|
            \leq B_1 + B_2 \cdot (|E| + B_1) \\
        & = & B_2 \cdot |E| + B_1 \cdot (1+B_2)
            \approx  B_2 \cdot |E| + B_1
\end{eqnarray*}
since $B_2 \ll 1$.
In most cases, either $B_2 = 0$ (without mixed precision) or $B_2$ is the
machine epsilon of the output format. Letting $\sum_{i=1}^n x_i \cdot y_i$
be the exact dot product, $B_1$ will either be
\[
   B_{11} = f(n) \cdot \epsilon \cdot \sum_{i=1}^n | x_i |  \cdot  | y_i |
\]
if floating point arithmetic (with machine precision $\epsilon$) is used 
to compute the dot product, or
\[
   B_{12} = g(n) \cdot \epsilon \cdot \max_i | x_i |  \cdot  \max_i | y_i |
\]
if some kind of block floating point is used, including the technique
exploiting integer arithmetic in arxiv:2306.11975v3.
The functions $f(n)$ and $g(n)$ are often $n$ in the
worst case, but can also be supplied by the sparse BLAS
developer.

It is not difficult  to see how to generate vectors $x$ and $y$ where $B_{11} \ll  B_{12}$,
to distinguish these two cases. More thought is required to design
vectors $x$ and $y$ where different variants of block floating point will have
errors closer to $B_{12}$ than $B_{11}$.

Other issues to possibly consider are (1) when $A$ in $A \cdot x$ is stored
in CSC format using block floating point, so that the inner loop is doing a
sparse axpy, and (2) when the common exponent in block floating point is
shared over more than 1 row or 1 column of $A$.

\subsubsection{Exception Handling}

Other numerical properties to be tested include the range of numbers
that can be handled without over/underflow, and whether NaNs and
Infs propagate as expected (arxiv:2207.09281).

One difference between the dense BLAS and sparse BLAS
is exception propagation with implicit zeros.
Implicit zeros in a sparse matrix A should probably not create NaNs when (not) performing
the multiplication A(i,j)*x(j) = 0 * Inf or 0 * NaN. This seems consistent with user expectations,
and analogous to how we treat alpha = 0 or beta = 0. But if an optimization like register blocking
turns an implicit zero into an explicit zero, then the NaN might propagate. I think this is ok for
the default behavior, possibly propagating exceptions "too much" but with the best performance.
We could discuss alternatives, like scanning x (in A*x) for Infs and NaNs, and doing something
more carefully in the very rare event that one appears, and providing this as a user selectable option.
Similar considerations apply to masked operations, if we choose to include those. We could also consider a third
user option, guaranteed bitwise-reproducility, including
exception propagation, which could be much slower, but
useful for debugging.

We should also consider how to handle zeros in x in TRSV: solving T*x = b. In the old reference BLAS, TRSV sometimes did and
sometimes did not check for zeros in x to avoid the work of multiplying the i-th column of T by x(i)=0.
These x(i)=0 could result either from trailing/leading zeros in b when T is upper/lower triangular,
or from exact numerical cancellation (much rarer). In the dense case we proposed disallowing
checking for zeros in x, and providing a simple routine to count the number of leading/trailing
zeros in b in case the user wanted to take advantage of this optimization. We should think about
these options when we discuss sparse TRSV, since x(i)=0 could occur more frequently in the sparse case. 

\section{Datasets}

What makes a choice of dataset a good choice?
\begin{itemize}
    \item There is a citation we can make
    \item There is a clear story/good argument connecting this data and its use (on a given kernel) to an application that people care about.
    \item The dataset is taken from another already existing benchmark
    \item The dataset exercises a particular extremal complexity case (e.g. expander graphs, or things coming from the lower bounds group)
    \item That the dataset should be representative of a real application of a  kernel (i.e. make sense together)
\end{itemize}

\subsection{Tensor Datasets}

	Tensors may be generated by a generator, or downloaded from a dataset
	online. The tensor datasets are listed here, together with their keys. For
	example, the key \texttt{SuiteSparse/Boeing/ct20stif.bs} refers to the
	\texttt{Boeing/ct20stif} matrix in the \texttt{SuiteSparse} collection. The
	\texttt{download.jl} script included with the suite will download and reformat
	datasets to binsparse or TTX format.

	\begin{itemize}
		\item[\texttt{SuiteSparse}] The University of Florida SuiteSparse Matrix Collection \cite{davis_university_2011}
		\item[\texttt{FROSTT}] The Formiddable Repository of Open Sparse Tensors and Tools \cite{smith_frostt_2017}
		\item[\texttt{ImageNet}] \url{https://www.kaggle.com/competitions/imagenet-object-localization-challenge/}
	\end{itemize}
        \hrl{I suggest fMRI datasets and HMP datasets, https://adni.loni.usc.edu/, which are kind of popular and known to be lacking this kind of benchmark}
\subsection{Tensor Generators}

    Tensor generators are listed here, together with a description of their
    parameters. All tensor generators that use random numbers additionally take a
	seed argument $s$. 

	Note that we will use the notation $A \sim D$ to denote tensor $A$ being generated from distribution $D$. For example, $A \sim Uniform_{Erdos-Renyi}((m \times n), nnz)$ means that $A$ is generated from an Erdos-Renyi generator.

	Often, instead of specifying the number of nonzeros, we may equivalently specify the density $\rho$.

\subsubsection{Erdos-Renyi}
	The Erdos-Renyi matrix generator selects a random tensor of size $m \times n$ with a given number of nonzeros $nnz$. The tensor is chosen uniformly at random from the set of tensors with $nnz$ nonzeros. All tensors have density $m * n / nnz$. We denote the distribution of this generator as:

	\[
		ErdosRenyi((m \times n), nnz)
	\]

	This is straightforwardly generalized to vectors or tensors by removing or inserting dimensions.
	
	Note that The Erdos-Renyi-Gilbert matrix generator selects a random tensor of size $m \times n$ where each nonzero is independently nonzero with a given probability $p$. This is a special case of the Erdos-Renyi distribution, where the number of nonzeros is a random variable with a binomial distribution.
	\[
		ErdosRenyiGilbert((m \times n), p) = ErdosRenyi((m \times n), \text{Binomial}(mn, p))
	\]

\subsubsection{RMAT}
    The RMAT matrix generator \cite{chakrabarti_r-mat_2004} creates graphs with
    power-law degree distributions, which presents an interesting challenge to
    load balancing for parallelization. The generator works by taking the
    Kronecker product of a seed matrix with itself several times to produce a
    matrix of probabilities of including nonzeros. An artifact of RMAT matrix
    generation is that heavy rows are often grouped together (a "staircase"
    pattern), so we choose to apply a random row/column permutation to the rows
    and columns. RMAT is used in the Graph500 and GAP benchmarks
    \cite{noauthor_graph_nodate, beamer_gap_2017}. Additionally, the tensor
    generalization (which takes Kronecker products of a tensor seed with itself)
    is used in the PASTA Benchmark Suite \cite{li_pasta_2019}.

    The RMAT matrix generator classically uses a generating matrix

	\[
		G = \left[\begin{array}{cc}
			A & B \\
			C & D \\
		\end{array}\right],
	\]
	though we will specify the entire tensor $S$, rather than the parameters
	$A$, $B$, $C$, $D$. Additionally, we will specify $r$, the number of times we join the seed matrix with itself in a Kronecker product. If the seed matrix is $m \times n$, the generated matrix is $m^r \times n^r$.

	We refer to the RMAT matrix generator with a seed $S$, factor $r$, and exactly $nnz$ nonzeros as:

	\[
		RMAT(S, r, nnz)
	\]

	We use GAP as a shorthand for the same parameters as the GAP benchmark \cite{beamer_gap_2017}.
	\[
	GAP = \left[\begin{array}{cc}
		0.57 & 0.19 \\
		0.19 & 0.05 \\
	\end{array}\right]
	\]
\subsubsection{Markov Clustering}
Steps of the MCL Algorithm
\begin{enumerate}

\item The input is an adjacency matrix $A$, representing the graph.
Initialization

\item Initialize matrix $M$ to $M_0$, the canonical transition matrix $M = (A + I) / D$ where $D$ is the degree matrix.


\item Expand: Perform matrix multiplication $M = M * M$.


\item Inflate: Apply inflation by raising each element in matrix $M$ to the power of the inflation parameter $r$ (typically 2), then re-normalize the columns.
Prune

\item Remove the matrix entries close to zero to save memory and speed up convergence.
Convergence Check

\item Check if the matrix M has converged. If not, return to the "Expand" step. If yes, proceed to output.
Output clusters

\item Extract the clusters, where each cluster corresponds to a group of nodes more interconnected among themselves than with the rest of the graph.
     
\end{enumerate}
The relationship to the spGEMM operation (sparse General Matrix Multiply) is primarily in the "Expand" step. GEMM is a basic operation in linear algebra that computes the product of two matrices and is central to many algorithms in numerical analysis, including those used in cluster analysis like the MCL. The matrix multiplication in the "Expand" step is essentially a GEMM operation where the current matrix M is multiplied by itself to simulate random walks through the graph, which are key to identifying the clusters. This step can be computationally intensive, and efficient GEMM operations are crucial for the MCL algorithm's performance, especially on large graphs.
%\hrl{This is implemented at https://github.com/SparseRooflineBenchmark/SparseRooflineBenchmark/issues/4#issuecomment-1979101102, there seems to be one graduate student trying to gather some more real-world examples.}
\section{Kernels}

\subsection{SpMV}
Sparse Matrix - Dense Vector multiplication (SpMV)
\subsubsection{Input Arguments}

\begin{itemize}
	\item[$m$:] a positive integer.
	\item[$n$:] a positive integer.
	\item[$A$:] an $m \times n$ sparse matrix.
	\item[$x$:] an $n$-length dense vector.
\end{itemize}

\subsubsection{Output Arguments}

\begin{itemize}
	\item[$y$:] an $m$-length dense vector.
\end{itemize}

\subsubsection{Computation}

$$y_i = \sum_{j}A_{ij}x_j$$

\subsubsection{Benchmark Stages (Provide timings for each stage)}

SpMV is frequently used repeatedly on the same matrix, making it the
subject of many optimizations which are independent of the vector $x$.  We also recognize that the performance of sparse matrix-vector multiply is highly dependent on the input matrix format. Thus, 
we separate the benchmark into two stages:

\begin{itemize}
\item \textbf{(Stage 1)}
	The kernel may read $m$, $n$, and $A$.
\item \textbf{(Stage 2)}:
	The kernel may read $x$.
\end{itemize}

This separation into stages allows us to account for the cost of e.g. reformatting in Stage 1 separately from the cost of the multiply in Stage 2.

\subsubsection{Datasets}

\begin{itemize}
	\item \texttt{spmv/erdos}: Random networks with sizes chosen logarithmically in the range 10,000 to 1,000,000 and 1,000,000 nonzeros. CSR format.
	\begin{multline*}
		m \sim 10^{Uniform(3:6)}, \quad n \sim 10^{Uniform(3:6)} \\
			A \sim ErdosRenyi\left(m\times n, nnz=10^6\right), \quad x \sim rand(m)
	\end{multline*}
	\item \texttt{spmv/GAP10}: Random GAP kronecker networks at 1 percent
	density. CSR format.
	\[
		A \sim RMAT\left(r=27, S = GAP, \rho = 0.01\right), \quad x \sim rand(2^{27})
	\]
	\item \texttt{spmv/OSKI}: The sparse matrices used in evaluation of
	``Performance Optimizations and Bounds for Sparse Matrix-Vector Multiply'' \cite{vuduc_performance_2002}. CSR format.
\end{itemize}

\subsection{SpMSpV}
Sparse Matrix - Sparse Vector multiplication (SpMV)
\subsubsection{Input Arguments}

\begin{itemize}
	\item[$m$:] a positive integer.
	\item[$n$:] a positive integer.
	\item[$A$:] an $m \times n$ sparse matrix.
	\item[$x$:] a length-$n$ sparse vector.
\end{itemize}

\subsubsection{Output Arguments}

\begin{itemize}
	\item[$y$:] an $m$-length dense vector.
\end{itemize}

\subsubsection{Computation}

$$y_i = \sum_{j}A_{ij}x_j$$

\subsubsection{Benchmark Stages (Provide timings for each stage)}

SpMSpV is also used repeatedly on the same matrix, so we separate into the same stages as SpMV:

\begin{itemize}
\item \textbf{(Stage 1)}
	The kernel may read $m$, $n$, and $A$.
\item \textbf{(Stage 2)}:
	The kernel may read $x$.
\end{itemize}

This separation into stages allows us to account for the cost of e.g. reformatting in Stage 1 separately from the cost of the multiply in Stage 2.

\subsubsection{Datasets}

Benchmarks for SpMSpV often vary the sparsity of the vector $x$ with the same data. We define the following settings for x, based on the observations in \cite{azad_work-efficient_2017}.
\begin{tabular}{|c|c|}\hline
	\texttt{x-density} & value\\
	\hline
	hypersparse & $nnz = 10$\\
	sparse & $\rho = 0.002$\\
	lesssparse & $\rho = 0.05$\\\hline
\end{tabular}

\begin{itemize}
	\item \texttt{spmspv/erdossparse/\$x-density}: Random networks with sizes chosen logarithmically in the range 10,000 to 100,000 and 1\% density. Vectors are also chosen uniformly randomly. CSR format.
	\begin{multline*}
		m \sim 10^{Uniform(3:6)}, \quad n \sim 10^{Uniform(3:6)} \\
			A \sim ErdosRenyi\left(m\times n, \rho=0.01\right), \quad x \sim  ErdosRenyi\left(n, \texttt{\$x-density}\right)
	\end{multline*}
	\item \texttt{spmv/GAP10/\$x-density}: Random GAP kronecker networks at 1 percent
	density. Vectors are also chosen uniformly randomly. CSR format.
	\[
		A \sim RMAT\left(r=27, S = GAP, \rho = 0.01\right), \quad x \sim  ErdosRenyi\left(n, \texttt{\$x-density}\right)
	\]
	\item \texttt{spmv/azad/\$x-density}: The sparse matrices used in evaluation of
	``A Work-Efficient Parallel Sparse Matrix-Sparse Vector Multiplication Algorithm'' \cite{azad_work-efficient_2017}. Vectors are also chosen uniformly randomly. CSR format.
	\[
		A \sim RMAT\left(r=27, S = GAP, \rho = 0.01\right), \quad x \sim  ErdosRenyi\left(n, \texttt{\$x-density}\right)
	\]
\end{itemize}

\subsection{SpMM}
Sparse Matrix Matrix multiplication (SpMM)
\subsubsection{Input Arguments}

\begin{itemize}
	\item[$m$:] a positive integer.
	\item[$n$:] a positive integer.
	\item[$d$:] a positive integer.
	\item[$A$:] an $m \times k$ sparse matrix.
	\item[$X$:] an $k \times d$ dense matrix.
\end{itemize}

\subsubsection{Output Arguments}

\begin{itemize}
	\item[$Y$:] an $m \times d$ dense matrix.
\end{itemize}

\subsubsection{Computation}

$$Y_{ik} = \sum_{j}A_{ij}X_{jk}$$

\subsubsection{Benchmark Stages (Provide timings for each stage)}

For similar reasons to SpMV, we separate the benchmark into two stages:

\begin{itemize}
\item \textbf{(Stage 1)}
	The kernel may read $m$, $n$, $p$ and $A$.
\item \textbf{(Stage 2)}:
	The kernel may read $X$.
\end{itemize}

\subsubsection{Datasets}

	Since SpMM datasets have a dense dimension which can often be set independently
	of other dimensions in the matrix, we provide the following named aliases for the number of columns in the dense matrix \cite{yesil_dense_2022}.

    \begin{tabular}{|c|c|}\hline
        \texttt{width} & value\\
        \hline
        skinny & $d = 8$\\
        medium & $d = 32$\\
        wide & $d = 128$\\\hline
    \end{tabular}

\begin{itemize}
	\item \texttt{spmm/erdos/\$width}: Random networks with sizes chosen logarithmically in the range 10,000 to 1,000,000 and 1,000,000 nonzeros. CSR format.
	\begin{multline*}
		m \sim 10^{Uniform(3:6)}, \quad n \sim 10^{Uniform(3:6)} \\
			A \sim ErdosRenyi\left(m\times n, nnz=10^6\right), \quad X \sim rand(m, d)
	\end{multline*}
	\item \texttt{spmm/GAP10/\$width}: Random GAP kronecker networks at 10 percent
	density. CSR format.
	\[
		A \sim RMAT\left(r=27, S = GAP, \rho = 0.1\right), \quad X \sim rand(2^{27}, d)
	\]
	\item \texttt{spmm/OSKI/\$width}: The sparse matrices used in evaluation of
	``Performance Optimizations and Bounds for Sparse Matrix-Vector Multiply'' \cite{vuduc_performance_2002}. CSR format.
\end{itemize}



\subsection{SpGEMM}
Sparse Matrix, Sparse Matrix multiplication (SpGEMM)
\subsubsection{Input Arguments}

\begin{itemize}
	\item[$m$:] a positive integer.
	\item[$n$:] a positive integer.
	\item[$p$:] a positive integer.
	\item[$A$:] an $m \times p$ sparse matrix.
	\item[$B$:] an $p \times n$ sparse matrix.
\end{itemize}

\subsubsection{Output Arguments}

\begin{itemize}
	\item[$C$:] an $m \times n$ dense matrix.
\end{itemize}

\subsubsection{Computation}

$$C_{ij} = \sum_{k}A_{ik}B_{kj}$$

\subsubsection{Benchmark Stages (Provide timings for each stage)}

This benchmark is not separated into stages, most applications of SpGemm do not
reuse either matrix.

\begin{itemize}
\item \textbf{Compute Stage}
	The kernel may read $m$, $n$, $p$, $A$, and $B$.
\end{itemize}

\subsubsection{Datasets}

\begin{itemize}
	\item \texttt{spgemm/erdos}: Two Random networks with sizes chosen logarithmically in the range 10,000 to 1,000,000 and 1,000,000 nonzeros. CSR format.
	\begin{multline*}
		m \sim 10^{Uniform(3:6)}, \quad n \sim 10^{Uniform(3:6)} \\
			A \sim ErdosRenyi\left(m\times n, nnz=10^6\right)
			B \sim ErdosRenyi\left(m\times n, nnz=10^6\right)
	\end{multline*}
	\item \texttt{spgemm/AA/GraphSquaring}: Square Random GAP kronecker networks at 10 percent density. CSR format.
	\[
		A \sim RMAT\left(r=27, S = GAP, \rho = 0.1\right), \quad B = A
	\]
	\item \texttt{spgemm/AA/GAMMA}: The square sparse matrices used in evaluation of
	``Gamma: leveraging Gustavsonâ€™s algorithm to accelerate sparse matrix multiplication'' \cite{zhang_gamma_2021}. In this case, $B = A$. 
	\item \texttt{spgemm/ATA/GAMMA}: All sparse matrices used in evaluation of
	``Gamma: leveraging Gustavsonâ€™s algorithm to accelerate sparse matrix multiplication'' \cite{zhang_gamma_2021}. In this case, $B = A^T$. CSR format.
\end{itemize}

\subsection{MTTKRP}
Matricized Khatri-Rao Product (MTTKRP)
\subsubsection{Input Arguments}

\begin{itemize}
	\item[$m, n, p, ...$:] positive integers.
	\item[$R$:] a positive integer, the rank.
	\item[$X$:] an $m \times n \times k ...$ sparse tensor, with a dataset-dependent number of dimensions.
	\item[$B, C, ...$:] a collection of $n \times R$ dense matrices, numbering one less than the number of dimensions in $X$.
\end{itemize}

\subsubsection{Output Arguments}

\begin{itemize}
	\item[$A$:] an $m \times R$ dense matrix.
\end{itemize}

\subsubsection{Computation}

$$A_{il} = \sum_{jk...}X_{ijk}(B_{jl}C_{kl}...)$$

\subsubsection{Benchmark Stages (Provide timings for each stage)}

MTTKRP is frequently used repeatedly on the same tensor during canonical polyadic decomposition. In the factorization algorithm, the dense matrices $A$, $B$ and $C$ swap roles, and are each updated in turn and can be thought of as different every time. Thus, we provide the dense matrices in a second stage.

\begin{itemize}
\item \textbf{(Stage 1)}
	The kernel may read $R$, $X$ and $m, n, p, ...$
\item \textbf{(Stage 2)}:
	The kernel may read $B, C, ...$.
\end{itemize}

\subsubsection{Datasets}

	Since MTTKRP datasets have a dense dimension (the tensor rank $R$) which can be set independently
	of other dimensions in the matrix, we provide the following named aliases for the number of columns in the dense matrices \cite{kolda_tensor_2009,li_sparse_2020}.

    \begin{tabular}{|c|c|}\hline
        \texttt{rank} & value\\
        \hline
        skinny & $d = 16$\\
        wide & $d = 64$\\\hline
    \end{tabular}

\begin{itemize}
	\item \texttt{mttkrp/erdos3/\$rank}: Random 3-tensors with sizes chosen logarithmically in the range 1,000 to 100,000 and 1,000,000 nonzeros. CSF format.
	\begin{multline*}
		m \sim 10^{Uniform(3:5)}, \quad n \sim 10^{Uniform(3:5)}, \quad k \sim 10^{Uniform(3:5)}\\
			A \sim ErdosRenyi\left(m\times n \times k, nnz=10^6\right), \quad B \sim rand(n, d),  \quad C \sim rand(k, d)
	\end{multline*}
	\item \texttt{mttkrp/PASTA/\$rank}: The sparse tensors proposed in the PASTA benchmark suite \cite{li_pasta_2019}. CSF format.
\end{itemize}

%Here is the bibliography:
\bibliographystyle{plainurl}
\bibliography{main}

\end{document}
