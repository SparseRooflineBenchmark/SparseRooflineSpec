\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{hyperref}

\title{The Sparse Roofline Benchmark Specification}
\author{Editors: Willow Ahrens, Olivia Hsu, (Please add your name!)}
\date{June 2023}

\begin{document}

\maketitle

\section{Introduction}

\section{Tensor Formats}

To enable cross-language compaitibility, all tensors generators or dataset downloads will produce tensor files in either the TTX (text-based) or Binsparse HDF5 (binary) tensor formats. The precise filenames and paths will be specified along with the benchmark kernel that uses them. For example, one might use the RMAT matrix generator included with the suite to generate inputs, stored in \texttt{spmv/RMAT/A.ttx} and \texttt{spmv/RMAT/x.ttx}

\section{Timing}

Each benchmark is separated into stages based on what information the application is allowed to read at each stage. For example, to enable a clear distinction between matrix preprocessing and runtime in SpMV kernels, the first stage allows access to just the matrix. Any vector-agnostic preprocessing can be timed with the first stage. The second stage allows access to the vector, and thus computation of the output. We recognize that many divisions between stages are possible; we have selected separations into stages that we believe are meaningful for applications.

Applications may choose to report either the minimum or the geometric mean of a predefined number of exections, and should also report whether the cache was cleared at the start of each exectution.

\section{Correctness and Validation}

\subsection{Exception Handling}
\begin{itemize}
\item NaN * Explicit zero = NaN

\item NaN * implicit zero = implicit zero
\end{itemize}

\subsection{Comparison to Reference Implementation}
\begin{enumerate}
    \item Allow reorganization of algorithm up to equivalence under arbitrary semirings (treat unsupported functions as uninterpreted) (e.g. thou shalt not do Strassen)
    \item Allow reorganization of algorithm up to equivalence under infinite precision
    \item Some combination of the above
    \item Anything you want up to declared empirical accuracy
\end{enumerate}

If algorithm is iterative, run steps until tolerance
If algorithm is non-iterative, report achieved empirical accuracy compared to double.

\section{In Memory Formats}


\section{Datasets}

What makes a choice of dataset a good choice?
\begin{itemize}
    \item There is a citation we can make
    \item There is a clear story/good argument connecting this data and its use (on a given kernel) to an application that people care about.
    \item The dataset is taken from another already existing benchmark
    \item The dataset exercises a particular extremal complexity case (e.g. expander graphs, or things coming from the lower bounds group)
    \item That the dataset should be representative of a real application of a  kernel (i.e. make sense together)
\end{itemize}

\subsection{Tensor Datasets}

	Tensors may be generated by a generator, or downloaded from a dataset
	online. The tensor datasets are listed here, together with their keys. For
	example, the key \texttt{SuiteSparse/Boeing/ct20stif.bs} refers to the
	\texttt{Boeing/ct20stif} matrix in the \texttt{SuiteSparse} collection. The
	\texttt{download.jl} script included with the suite will download and reformat
	datasets to binsparse or TTX format.

	\begin{itemize}
		\item[\texttt{SuiteSparse}] The University of Florida SuiteSparse Matrix Collection \cite{davis_university_2011}
		\item[\texttt{FROSTT}] The Formiddable Repository of Open Sparse Tensors and Tools \cite{smith_frostt_2017}
		\item[\texttt{ImageNet}] \url{https://www.kaggle.com/competitions/imagenet-object-localization-challenge/}
	\end{itemize}

\subsection{Tensor Generators}

    Tensor generators are listed here, together with a description of their
    parameters. All tensor generators that use random numbers additionally take a
	seed argument $s$. 

\subsubsection{RMAT}
    The RMAT matrix generator \cite{chakrabarti_r-mat_2004} creates graphs with
    power-law degree distributions, which presents an interesting challenge to
    load balancing for parallelization. The generator works by taking the
    Kronecker product of a seed matrix with itself several times to produce a
    matrix of probabilities of including nonzeros. An artifact of RMAT matrix
    generation is that heavy rows are often grouped together (a "staircase"
    pattern), so we choose to apply a random row/column permutation to the rows
    and columns. RMAT is used in the Graph500 and GAP benchmarks
    \cite{noauthor_graph_nodate, beamer_gap_2017}. Additionally, the tensor
    generalization (which takes Kronecker products of a tensor seed with itself)
    is used in the PASTA Benchmark Suite \cite{li_pasta_2019}.

    The RMAT matrix generator classically uses a generating matrix

	\[
		G = \left[\begin{array}{cc}
			A & B \\
			C & D \\
		\end{array}\right],
	\]
	though we will specify the entire tensor $G$, rather than the parameters
	$A$, $B$, $C$, $D$. Additionally, we will specify $r$, the number of seed
	matrices which should be joined with a Kronecker product.

\section{Kernels}

\subsection{SpMV}

\subsubsection{Input Arguments}

\begin{itemize}
	\item[$m$:] a positive integer.
	\item[$n$:] a positive integer.
	\item[$A$:] an $m \times n$ matrix in CSR format.
	\item[$x$:] an $n$-length vector in dense format.
\end{itemize}

\subsubsection{Output Arguments}

\begin{itemize}
	\item[$y$:] a dense vector where $y_i = \sum_{ij}A_{ij}x_j$.
\end{itemize}

\subsubsection{Benchmark Stages (Provide timings for each stage)}

SpMV is frequently used repeatedly on the same matrix. Therefore, it is the
subject of many optimizations which are independent of the vector $x$. Thus, 
we separate the benchmark into two stages:

\begin{itemize}
\item \textbf{(Stage 1)}
	The kernel may read $m$, $n$, and $A$.
\item \textbf{(Stage 2)}:
	The kernel may read $x$.
\end{itemize}

\subsubsection{Datasets}

\begin{itemize}
	\item[\texttt{spmv/kronecker10}:] 100 random kronecker networks at 10 percent sparsity, using the
	same parameters as GAP and Graph500 \cite{noauthor_graph_nodate}.
	\begin{itemize}
		\item[\texttt{spmv/kronecker10/A}:] $\texttt{RMAT}\left(r=27, G = \left[\begin{array}{cc}
			0.57 & 0.19 \\
			0.19 & 0.05 \\
		\end{array}\right]\right)$
		\item[\texttt{spmv/kronecker10/x}:] $\texttt{rand}\left(m = 2^{27}\right)$
	\end{itemize}
	\item[\texttt{spmv/OSKI}:] the sparse matrices used in evaluation of
	``Performance Optimizations and Bounds for Sparse Matrix-Vector Multiply'' \cite{vuduc_performance_2002}.
\end{itemize}

Everything after this line is a draft:

\noindent\rule{\textwidth}{1pt}

\subsubsection{Variations}
    Also include SpMV transpose.

    Mention $A=A^T$, possible additional optimizations,
    including``Fast Bilinear Algorithms for Symmetric Tensor 
    Contractions'', E. Solomonik, J. Demmel, Computational Methods
    in Applied Mathematics, Feb 2020.
    \newline
    https://doi.org/10.1515/cmam-2019-0075



\subsection{Iterative (Jacobi? PCG?) Solve}
	Input (Stage 1):
		Matrix given (e.g. “Boeing/ct20stif” or power-law graph laplacian)
	Input (Stage 2):
		Vector (e.g. uniform random dense vector)
	Compute:
		$y_j = A_ij * x_j$
	Timings:
Report time start stage 1 to start stage 2
Report time start stage 2 to end
Report aggregate time
Some combination of:
1. Allow reorganization of algorithm up to equivalence under arbitrary semirings (treat unsupported functions as uninterpreted) (e.g. thou shalt not do Strassen)
2. Allow reorganization of algorithm up to equivalence under infinite precision
2.5 (2 and 3)
3. Anything you want up to declared empirical accuracy
If algorithm is iterative, run steps until tolerance
If algorithm is non-iterative, report achieved empirical accuracy compared to double.

\subsection{SpMSpV}
``Sparse-Sparse Matrix-Vector Multiply'' i.e. the matrices and vectors are all sparse.
$$y_i += A_{i,j} * x_j$$

\subsection{SpMM (SpGEMM?)}
	Input (Stage 1):
		Matrix
	Input (Stage 2):
		Vector
	Compute:
		$y_ij = A_ik * x_kj$
	Timings:
Report time start stage 1 to start stage 2
Report time start stage 2 to end
Report aggregate time

SpGEMM from multi-source BFS:
	Computation: Compute BFS parents starting from a particular set of vertices.
	Distribution: If the graph is fixed, you can precompute the BFS parents (not interesting). Thus, this problem is only well-posed on graph generators. I’m not sure if the set of vertices matters, could pick randomly.

SpGEMM from setup phase of algebraic multigrid:
	Computation: Compute $R_lA_lP_L$ as described in \url{https://doi.org/10.1145/3571157}.
	Distribution: matrices described in \url{https://doi.org/10.1145/3571157}
SpGEMM with various levels of size prediction difficulty and output reuse:
	Computation: Compute C = A*B
	Distribution: Generate A and B (using RMAT, perhaps?) so that expected number of nonzeros in A and B remain fixed but expected number of nonzeros in C can be very high, very low, or highly variable

\subsection{ATA}

\subsection{MTTKRP}
	Computation: A single MTTKRP 
	Distribution: The dense matrices are random, but the tensor can be fixed or random.


\subsection{Candecomp}
	Computation: Some fixed number of MTTKRPs of the same tensor in alternating transposition orders on the same factor matrices
	Distribution: The tensor can be fixed or random here as well.
Stencil (Same as SpMV, but matrix is a stencil)
	Computation: SpMV or Congugate gradient
	Distribution: Matrix is a toeplitz matrix for a particular stencil, vector is random

\subsection{FFT}
	Computation: SpMV
	Distribution: Matrix is an FFT matrix, vector is random

\subsection{Dynamic PageRank}
	Computation: Compute Pagerank of a matrix
	Stage 1.
		The first (n - d) rows and columns of a matrix are given
	Stage 2.
		The final d rows and columns are given
	Computation:
		Do 20 iterations of pagerank
	Notes: This incentivizes the implementation to use the first portion of the matrix to accelerate the computation in stage 2.
(Gilbert: This is just a power iteration? Willow: It is. I was trying to emphasize the dynamic graph kernel problem, where updates to a graph are received in real time and we are trying to maintain some statistics on the graph as the updates come in. Gilbert: I'm not sure page-rank or similar are incrementalized in practice?  Maybe.  Maybe worth considering social network clustering algorithms here instead?)

\subsection{Quantum Circuits}
	Computation: Compute the distribution of the quantum circuits outputs
	Distribution: 1 Tensor for each circuit element (10s-100s total)
Transformer Model:
	Computation: Compute the output of a single encoder block in a transformer model with k attention heads
	Distribution: Randomly initialized weights with some sparsification algorithm applied
 
 \begin{align*}
Q[S, E, K] &= \sum_C(I[S, C] * WQ[C, E, K])\\
K[S, E, K] &= \sum_C(I[S, C] * WK[C, E, K])\\
V[S, E, K] &= \sum_C(I[S, C] * WV[C, E, K])\\
AP[S, S’, K] &= \sum_E(Q[S, E, K] * K[S’, E, K]) / 8\\
Z[S, E, K] &= \sum_S’(Sigmoid(AP[S,S’,K]) * V[S’, E, K])\\
CZ[S, E] &= \sum_k(Z[S, E, K] * WZ[K,E])\\
FF[S,FF] &= Relu(CZ[S,E] * WFF1[E,FF])\\
O[S,E] &= Relu(FF[S,FF] * WFF2[FF,E])\\
\end{align*}

\subsection{CountSat}
	Computation: Compute the number of satisfying solutions for a sat problem. This is a tensor contraction along each variable of the product of all clause tensors according to matching variables.
	Distribution: Randomly generated sat clauses with variables of length 3-10 (Existing Sat datasets? Are random sat problems unsatisfiable w.h.p.?)

\subsection{Graph Neural Networks}
	Computation: Compute one iteration of message passing
	Distribution: Random graph models, random sparsified weights

% WX, BX is a simple affine transform, but it could be NN layers instead
$NeighborMessages[u, v, k’] = A[u, v]*(Sum_k(X[v, k]*WX[k, k’]) +BX[k’])  $

$AggNeighborMessages[u, k’] = Sum_v(NeighborMessage[u, v, k’])$

% This could also be a neural network
$NewX[u, k] = softmax_k(WX’[u,k](X[u, k] +AggNeighborMessages[u,k]) + BX’[k]) $

\subsection{SubGraph Counting}
	Computation: Compute the number of subgraphs of a particular kind (triangle, flower, petal, etc.)
	Distribution: Random graph models, directed vs undirected subvariants

\section{Future Benchmark Verticals}

\subsection{Quantum Chemistry Stuff}
Oh cool, quantum chemistry stuff!

\subsection{Morgue from Earlier Notes}

Produce a List of all Computation Kernels (Acronyms & Definitions)
SpMV? SpMM? SpDM3? SpMSpV? SPGEMM? SpLU? SpTRSV/M?

SpMV:
	“Sparse Matrix-Vector Multiply”
Dense x,y, Sparse A
y[i] += A[i, j] * x[j]
SpMSpV:
	“Sparse-Sparse Matrix-Vector Multiply”
Sparse A, x, y
y[i] += A[i, j] * x[j]
SpGEMM:
	“Sparse-Sparse Matrix Multiply”
Sparse C, A, B, often all matrices are square
C[i, j] += A[i, k] * B[k, j]
Masked SpGEMM:
	Sparse (sometimes Boolean) S, C, Sparse A, B
C[i, j] += S[i, j] * A[i, k] * B[k, j]
SDDMM:
	“Sampled Dense Dense Matrix Multiply”
	Sparse (sometimes Boolean) S, C, Dense A, B
C[i, j] += S[i, j] * A[i, k] * B[k, j]

SpMM:
	“Sparse Matrix Multiply”
Dense C, B, Sparse A, often J small constant
C[i, j] += A[i, k] * B[k, j]

MTTKRP:
	“Matricized Tensor Times Khatri-Rao Product”
Sparse B, Dense A, C, D
A[i, j] += B[i, k, l] * C[k, j] * D[l, j]
SpTTM:
	“Sparse Tensor Times Matrix”
Sparse A, B, Dense C, usually K >> L or L >> K
A[i, j, k] += B[i, j, l] * C[k, l]
SpMTTKRP:
Sparse A, B, C, D
A[i, j] += B[i, k, l] * C[k, j] * D[l, j]
SpLU:
	“Sparse LU Factorization”
Sparse A, produce permutations P, Q, sparse lower triangular L, and sparse upper triangular U
such that PAQ=LU
SpTRSV:
	“Sparse Triangular Solve”
	Sparse triangular matrix A, dense vector y, produce dense vector x such that Ax = y
SpTRSM:
	Sparse triangular matrix A, dense matrix Y, produce dense matrix X such that AX = Y

Willow: I’ve taken a stab at listing some kernel definitions. One might group them broadly by:
	1 argument sparse: SpMV, SpMM, SDDMM,MTTKRP, SpTTM
	2+ argument sparse: SpGEMM, SpMSpV, SpMTTKRP
	Loop carry dependency: SpLU, SpTRSV, SpTRSM

%Here is the bibliography:
\bibliographystyle{plain}
\bibliography{main}

\end{document}
